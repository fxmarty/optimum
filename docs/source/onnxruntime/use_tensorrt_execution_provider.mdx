## Installation

The easiest way to make use of TensorRT as the execution library for models optimized through Optimum is to use Docker images [DETAILS TODO].

However, if you want to use Optimum-onnxruntime with TensorRT in a local environment, the easiest way is to follow the NVIDIA documentation to install:
* CUDA toolkit: https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html
* cuDNN: https://docs.nvidia.com/deeplearning/cudnn/install-guide/index.html
* TensorRT: https://docs.nvidia.com/deeplearning/tensorrt/install-guide/index.html

For TensorRT, we recommand the Tar File Installation method. Alternatively, TensorRT may be installable through the `nvidia-tensorrt` pip wheel available following [these instructions](https://github.com/microsoft/onnxruntime/issues/9986).

The following environment variables need to be set for ONNX Runtime to detect TensorRT installation:
```
export CUDA_PATH=/usr/local/cuda
export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/cuda-11.7/lib64:/path/to/TensorRT-8.x.x/lib
```

# How to use TensorRT execution provider through Optimum?

For non-quantized models, the use is straightforward, by simply using the `execution_provider` option in `ORTModel.from_pretrained()`. For example:

```python
from optimum.onnxruntime import ORTModelForSequenceClassification

ort_model = ORTModelForSequenceClassification.from_pretrained(
    "distilbert-base-uncased-finetuned-sst-2-english",
    from_transformers=True,
    execution_provider="TensorrtExecutionProvider",
)
```

# Use quantized models with TensorRT

TensorRT supports only models using **static** quantization with **symmetric** weights and activation. Thus, for model quantized through Optimum to later be consumed by TensorRT, the following configuration needs to be passed when doing calibration for static quantization:

```python
qconfig = QuantizationConfig(
    ...,
    is_static=True,
    activations_symmetric=True,
    weights_symmetric=True,
    qdq_dedicated_pair=True,
    qdq_add_pair_to_weight=True
)
```

The parameter `qdq_dedicated_pair=True` is required by TensorRT, that expects a single node after each `QuantizeLinear` + `DequantizeLinear` (QDQ) pair.

The parameter `qdq_add_pair_to_weight=True` is also required by TensorRT, that consumes a graph where the weights are stored in float32 with a QDQ pair. Normally, weights would be stored in int8 format and only a `DequantizeLinear` would be applied on the weights. As such, the storage savings from quantization can not be leveraged when we expect to later use the quantized `.onnx` model with TensorRT.

After performing static quantization [LINK], we can load the model into the `ORTModel` class using TensorRT as the execution provider. The fact that int8 operations are used needs to be specified to TensorRT, and ONNX Runtime graph optimization need to be disabled for the model to be consumed rather by TensorRT.

```python
session_options = onnxruntime.SessionOptions()
session_options.graph_optimization_level = onnxruntime.GraphOptimizationLevel.ORT_DISABLE_ALL

model_name = "fxmarty/distilbert-base-uncased-finetuned-sst-2-english-int8-static-symmetric-dedicated-qdq-everywhere"
ort_model = ORTModelForSequenceClassification.from_pretrained(
    model_name,
    from_transformers=False,
    provider="TensorrtExecutionProvider",
    session_options=session_options,
    provider_options={"trt_int8_enable": True},
)
```

The model can then be used with the common transformers API to do inference.

For example, to wrap the model in a pipeline:
```python
from optimum.pipelines import pipeline
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained(model_name)

pipe = pipeline(task="text-classification", model=ort_model, tokenizer=tokenizer)
result = pipe("Both the music and visual were astounding, not to mention the actors performance.")
print(result)
```

## TensorRT limitations for quantized models

As highlighted in the previous section, TensorRT supports only a limited range of quantized model:
* Static quantization only
* Weights and activations quantization ranges are **symmetric**
* Weights need to be stored in float32 in the `.onnx` model, thus no storage space is possible.

In case `provider="TensorrtExecutionProvider"` is passed in cases where the model has not been quantized following these constraints, multiple errors may be raised, where error messages can be unclear.

## Observed time gains

Nvidia Nsight Systems tool can be used to profile the execution time on GPU.

