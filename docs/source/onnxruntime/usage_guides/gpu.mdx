# Use Nvidia GPU for inference through Optimum-onnxruntime

Although ONNX Runtime defaults to use CPU-only for inference, it is possible to make use of a Nvidia GPU by placing supported operations on GPU, while leaving unsupported one (if remaining) on CPU. In most cases, all operations can be placed on GPU, allowing to leverage the compute power.

ONNX Runtime provides an integration with two execution providers to make use of an Nvidia GPU, namely `CUDAExecutionProvider` and `TensorrtExecutionProvider`.

## CUDAExecutionProvider

### Installation



### Checking the installation is successful

Before going further, run the following sample code to check whether the install was successful:

```python

```

In case the printed result is `["CUDAExecutionProvider", "CPUExecutionProvider"]`, congratulations, the installation is successful!

### Use CUDAExecutionProvider with floating-point model

EXAMPLE

Gather, Unsqueeze, Concat use CPUExecutionProvider, all the rest CUDAExecutionProvider

### Use CUDAExecutionProvider with quantized models

### Observed time gains

## TensorrtExecutionProvider

### Installation

The easiest way to make use of TensorRT as the execution library for models optimized through Optimum is to use Docker images [DETAILS TODO].

However, in order to use Optimum-onnxruntime with TensorRT in a local environment, it is recommended to follow the NVIDIA documentation:
* CUDA toolkit: https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html
* cuDNN: https://docs.nvidia.com/deeplearning/cudnn/install-guide/index.html
* TensorRT: https://docs.nvidia.com/deeplearning/tensorrt/install-guide/index.html

For TensorRT, we recommend the Tar File Installation method. Alternatively, TensorRT may be installable through the `nvidia-tensorrt` pip wheel available following [these instructions](https://github.com/microsoft/onnxruntime/issues/9986).

The following environment variables need to be set for ONNX Runtime to detect TensorRT installation:
```
export CUDA_PATH=/usr/local/cuda
export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/cuda-11.7/lib64:/path/to/TensorRT-8.x.x/lib
```

### Checking the installation is successful

Before going further, run the following sample code to check whether the install was successful:

```python

```

In case the printed result is `["TensorrtExecutionProvider", "CUDAExecutionProvider", "CPUExecutionProvider"]`, congratulations, the installation is successful!

### Use TensorRT execution provider with floating-point models

For non-quantized models, the use is straightforward, by simply using the `execution_provider` argument in `ORTModel.from_pretrained()`. For example:

```python
from optimum.onnxruntime import ORTModelForSequenceClassification

ort_model = ORTModelForSequenceClassification.from_pretrained(
    "distilbert-base-uncased-finetuned-sst-2-english",
    from_transformers=True,
    execution_provider="TensorrtExecutionProvider",
)
```

Additionally, By passing the session option `log_severity_level = 0` (verbose), we see in the logs that ONNX Runtime can check whether all nodes are indeed placed on the TensorRT execution provider or not:

```
2022-09-22 14:12:48.371513741 [V:onnxruntime:, session_state.cc:1188 VerifyEachNodeIsAssignedToAnEp] All nodes have been placed on [TensorrtExecutionProvider]
```

### Use quantized models with TensorRT execution provider

TensorRT supports only models that use **static** quantization with **symmetric quantization** for weights and activations. Thus, for models quantized through Optimum to later be consumed by TensorRT, the following configuration needs to be passed when doing calibration for static quantization:

```python
qconfig = QuantizationConfig(
    ...,
    is_static=True,
    activations_symmetric=True,
    weights_symmetric=True,
    qdq_dedicated_pair=True,
    qdq_add_pair_to_weight=True
)
```

The parameter `qdq_dedicated_pair=True` is required by TensorRT, that expects a single node after each `QuantizeLinear` + `DequantizeLinear` (QDQ) pair.

The parameter `qdq_add_pair_to_weight=True` is also required by TensorRT, that consumes a graph where the weights are stored in float32 with a QDQ pair. Normally, weights would be stored in fixed point 8-bits format and only a `DequantizeLinear` would be applied on the weights. As such, the storage savings from quantization can not be leveraged when we expect to later use the quantized `.onnx` model with TensorRT.

In the code sample below, after performing static quantization, the resulting model is loaded into the `ORTModel` class using TensorRT as the execution provider. ONNX Runtime graph optimization need to be disabled for the model to be consumed and optimized by TensorRT, and the fact that int8 operations are used needs to be specified to TensorRT.

```python
session_options = onnxruntime.SessionOptions()
session_options.graph_optimization_level = onnxruntime.GraphOptimizationLevel.ORT_DISABLE_ALL

model_name = "fxmarty/distilbert-base-uncased-finetuned-sst-2-english-int8-static-symmetric-dedicated-qdq-everywhere"
ort_model = ORTModelForSequenceClassification.from_pretrained(
    model_name,
    from_transformers=False,
    provider="TensorrtExecutionProvider",
    session_options=session_options,
    provider_options={"trt_int8_enable": True},
)
```

The model can then be used with the common transformers API for inference, such as pipelines:
```python
from optimum.pipelines import pipeline
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained(model_name)

pipe = pipeline(task="text-classification", model=ort_model, tokenizer=tokenizer)
result = pipe("Both the music and visual were astounding, not to mention the actors performance.")
print(result)
```

### TensorRT limitations for quantized models

As highlighted in the previous section, TensorRT supports only a limited range of quantized model:
* Static quantization only
* Weights and activations quantization ranges are **symmetric**
* Weights need to be stored in float32 in the `.onnx` model, thus there is no storage space saving from quantization.

In case `provider="TensorrtExecutionProvider"` is passed and the model has not been quantized strictly following these constraints, various errors may be raised, where error messages can be unclear.

### Observed time gains

Nvidia Nsight Systems tool can be used to profile the execution time on GPU. Before profiling or measuring latency/throughput, it is a good practice to do a few **warmup steps**.

