# Use Nvidia GPU for inference through Optimum-onnxruntime

Although ONNX Runtime defaults to use CPU-only for inference, it is possible to place supported operations on an Nvidia GPU, while leaving unsupported ones (if remaining) on CPU. In most cases, all costly operations can be placed on GPU, allowing to leverage its compute power.

ONNX Runtime provides an integration with two execution providers to make use of an Nvidia GPU, namely `CUDAExecutionProvider` and `TensorrtExecutionProvider`.

In case you are using a vanilla model using floating-point arithmetic, both execution providers can be used. TensorRT generally has the best runtime performances, although it comes with its own caveats, notably limitations for inference with inputs having dynamic shape.

When deploying a quantized model to a GPU, `CUDAExecutionProvider` can not be used, and only models using static quantization can be executed on `TensorrtExecutionProvider`.

## CUDAExecutionProvider

### Installation

Provided the CUDA and cuDNN [requirements](https://onnxruntime.ai/docs/execution-providers/CUDA-ExecutionProvider.html#requirements) are satisfied, the install is then straightforward with `pip install optimum[onnxruntime-gpu]`. Beforehand, to avoid conflicts between `onnxruntime` and `onnxruntime-gpu`, make sure the package `onnxruntime` is not installed by running `pip uninstall onnxruntime`.

### Checking the installation is successful

Before going further, run the following sample code to check whether the install was successful:

```python
from optimum.onnxruntime import ORTModelForSequenceClassification
from transformers import AutoTokenizer

ort_model = ORTModelForSequenceClassification.from_pretrained(
    "philschmid/tiny-bert-sst2-distilled",
    from_transformers=True,
    provider="CUDAExecutionProvider",
)
print("providers:", ort_model.providers)

tokenizer = AutoTokenizer.from_pretrained("philschmid/tiny-bert-sst2-distilled")
inp = tokenizer("expectations were low, actual enjoyment was high", return_tensors="pt", padding=True)

result = ort_model(**inp)
print(result)
```

In case the printed result is `providers: ['CUDAExecutionProvider', 'CPUExecutionProvider']`, congratulations, the installation is successful!

In case the error `ValueError: Asked to use CUDAExecutionProvider as an ONNX Runtime execution provider, but the available execution providers are ['CPUExecutionProvider'].` is raised, something is off in your installation.

### Use CUDA execution provider with floating-point models

For non-quantized models, the use is straightforward, by simply using the `provider` argument in `ORTModel.from_pretrained()`. For example:

```python
from optimum.onnxruntime import ORTModelForSequenceClassification

ort_model = ORTModelForSequenceClassification.from_pretrained(
    "distilbert-base-uncased-finetuned-sst-2-english",
    from_transformers=True,
    provider="CUDAExecutionProvider",
)
```

Additionally, By passing the session option `log_severity_level = 0` (verbose), we can check whether all nodes are indeed placed on the CUDA execution provider or not:

```
import onnxruntime

session_options = onnxruntime.SessionOptions()
session_options.log_severity_level = 0

ort_model = ORTModelForSequenceClassification.from_pretrained(
    "distilbert-base-uncased-finetuned-sst-2-english",
    from_transformers=True,
    provider="CUDAExecutionProvider",
    session_options=session_options
)
```

printing:

```
2022-10-18 14:59:13.728886041 [V:onnxruntime:, session_state.cc:1193 VerifyEachN
odeIsAssignedToAnEp]  Provider: [CPUExecutionProvider]: [Gather (Gather_76), Uns
queeze (Unsqueeze_78), Gather (Gather_97), Gather (Gather_100), Concat (Concat_1
10), Unsqueeze (Unsqueeze_125), ...]
2022-10-18 14:59:13.728906431 [V:onnxruntime:, session_state.cc:1193 VerifyEachN
odeIsAssignedToAnEp]  Provider: [CUDAExecutionProvider]: [Shape (Shape_74), Slic
e (Slice_80), Gather (Gather_81), Gather (Gather_82), Add (Add_83), Shape (Shape
_95), MatMul (MatMul_101), ...]
```

Here this result is expected: all the costly MatMul are placed on the CUDA execution provider.

### Use CUDA execution provider with quantized models

It is not possible to use quantized models with CUDAExecutionProvider.

When using [ðŸ¤— Optimum dynamic quantization](quantization#dynamic-quantization-example), nodes as [`MatMulInteger`](https://github.com/onnx/onnx/blob/v1.12.0/docs/Operators.md#MatMulInteger), [`DynamicQuantizeLinear`](https://github.com/onnx/onnx/blob/v1.12.0/docs/Operators.md#DynamicQuantizeLinear) may be inserted in the computation graph, that can not be consumed by the CUDA execution provider. This is a limitation from ONNX Runtime.

When using [static quantization](quantization#static-quantization-example), the onnx computation graph will contain matrix multiplications and convolutions in floating-point arithmetic, along with Quantize + Dequantize operations to simulate quantization. In this case, although the costly matrix multiplications and convolutions will be run on the GPU, they will use floating-point arithmetic as the `CUDAExecutionProvider` can not consume the Quantize + Dequantize nodes to replace them by the operations using integer arithmetic.

### Observed time gains

Coming soon!

## TensorrtExecutionProvider

### Installation

The easiest way to use TensorRT as the execution library for models optimized through Optimum is with the available ONNX Runtime `TensorrtExecutionProvider`.

In order to use Optimum-onnxruntime with TensorRT in a local environment, it is recommended to follow the NVIDIA documentation to install:
* CUDA toolkit: https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html
* cuDNN: https://docs.nvidia.com/deeplearning/cudnn/install-guide/index.html
* TensorRT: https://docs.nvidia.com/deeplearning/tensorrt/install-guide/index.html

For TensorRT, we recommend the Tar File Installation method. Alternatively, TensorRT may be installable through the `nvidia-tensorrt` pip wheel available following [these instructions](https://github.com/microsoft/onnxruntime/issues/9986).

The following environment variables need to be set for ONNX Runtime to detect TensorRT installation:
```
export CUDA_PATH=/usr/local/cuda
export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/cuda-11.7/lib64:/path/to/TensorRT-8.x.x/lib
```

### Checking the installation is successful

Before going further, run the following sample code to check whether the install was successful:

```python
from optimum.onnxruntime import ORTModelForSequenceClassification
from transformers import AutoTokenizer

ort_model = ORTModelForSequenceClassification.from_pretrained(
    "philschmid/tiny-bert-sst2-distilled",
    from_transformers=True,
    provider="TensorrtExecutionProvider",
)
print("providers:", ort_model.providers)

tokenizer = AutoTokenizer.from_pretrained("philschmid/tiny-bert-sst2-distilled")
inp = tokenizer("expectations were low, actual enjoyment was high", return_tensors="pt", padding=True)

result = ort_model(**inp)
print(result)
```

In case the printed result has `providers: ['TensorrtExecutionProvider', 'CUDAExecutionProvider', 'CPUExecutionProvider']` and no error is raised, congratulations, the installation is successful!

In case the printed result has only `providers: ['CUDAExecutionProvider', 'CPUExecutionProvider']`, or the warning `Failed to create TensorrtExecutionProvider. Please reference https://onnxruntime.ai/docs/execution-providers/TensorRT-ExecutionProvider.html#requirements to ensure all dependencies are met.` is raised, something is off in your installation.

### Use TensorRT execution provider with floating-point models

For non-quantized models, the use is straightforward, by simply using the `provider` argument in `ORTModel.from_pretrained()`. For example:

```python
from optimum.onnxruntime import ORTModelForSequenceClassification

ort_model = ORTModelForSequenceClassification.from_pretrained(
    "distilbert-base-uncased-finetuned-sst-2-english",
    from_transformers=True,
    provider="TensorrtExecutionProvider",
)
```

[As previously for `CUDAExecutionProvider`](#use-cudaexecutionprovider-with-floatingpoint-model), by passing the session option `log_severity_level = 0` (verbose), we can check in the logs whether all nodes are indeed placed on the TensorRT execution provider or not:

```
2022-09-22 14:12:48.371513741 [V:onnxruntime:, session_state.cc:1188 VerifyEachNodeIsAssignedToAnEp] All nodes have been placed on [TensorrtExecutionProvider]
```

### Use TensorRT execution provider with quantized models

When it comes to quantized models, TensorRT supports only models that use **static** quantization with **symmetric quantization** for weights and activations. Thus, to be able to consume with TensorRT models quantized through ðŸ¤— Optimum, the following configuration needs to be passed when doing [static quantization](quantization#static-quantization-example):

```python
qconfig = QuantizationConfig(
    ...,
    is_static=True,
    activations_symmetric=True,
    weights_symmetric=True,
    qdq_dedicated_pair=True,
    qdq_add_pair_to_weight=True
)
```

The parameter `qdq_dedicated_pair=True` is required by TensorRT, that expects a single node after each `QuantizeLinear` + `DequantizeLinear` (QDQ) pair.

The parameter `qdq_add_pair_to_weight=True` is also required by TensorRT, that consumes a graph where the weights are stored in float32 with a QDQ pair. Normally, weights would be stored in fixed point 8-bits format and only a `DequantizeLinear` would be applied on the weights. As such, the storage savings from quantization can not be leveraged when we expect to later use the quantized `.onnx` model with TensorRT.

In the code sample below, after performing static quantization, the resulting model is loaded into the `ORTModel` class using TensorRT as the execution provider. ONNX Runtime graph optimization need to be disabled for the model to be consumed and optimized by TensorRT, and the fact that int8 operations are used needs to be specified to TensorRT.

```python
session_options = onnxruntime.SessionOptions()
session_options.graph_optimization_level = onnxruntime.GraphOptimizationLevel.ORT_DISABLE_ALL

model_name = "fxmarty/distilbert-base-uncased-finetuned-sst-2-english-int8-static-symmetric-dedicated-qdq-everywhere"
ort_model = ORTModelForSequenceClassification.from_pretrained(
    model_name,
    from_transformers=False,
    provider="TensorrtExecutionProvider",
    session_options=session_options,
    provider_options={"trt_int8_enable": True},
)
```

The model can then be used with the common ðŸ¤— Transformers API for inference and evaluation, such as pipelines:
```python
from optimum.pipelines import pipeline
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained(model_name)

pipe = pipeline(task="text-classification", model=ort_model, tokenizer=tokenizer)
result = pipe("Both the music and visual were astounding, not to mention the actors performance.")
print(result)
```

### TensorRT limitations for quantized models

As highlighted in the previous section, TensorRT supports only a limited range of quantized model:
* Static quantization only
* Weights and activations quantization ranges are symmetric
* Weights need to be stored in float32 in the `.onnx` model, thus there is no storage space saving from quantization.

In case `provider="TensorrtExecutionProvider"` is passed and the model has not been quantized strictly following these constraints, various errors may be raised, where error messages can be unclear.

### Observed time gains

Nvidia Nsight Systems tool can be used to profile the execution time on GPU. Before profiling or measuring latency/throughput, it is a good practice to do a few **warmup steps**.

Coming soon!
